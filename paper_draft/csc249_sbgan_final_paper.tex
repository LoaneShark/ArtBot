\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{\LaTeX\ Author Guidelines for CVPR Proceedings}

\author{Bradley Beyers\\
	Institution1\\
	Institution1 address\\
	{\tt\small bbeyers@u.rochester.edu}
	% For a paper whose authors are all at the same institution,
	% omit the following lines up until the closing ``}''.
	% Additional authors and addresses can be added with ``\and'',
	% just like the second author.
	% To save space, use either the email address or home page, not both
	\and
	Santiago Loane\\
	Institution2\\
	First line of institution2 address\\
	{\tt\small sloane@u.rochester.edu}
}


\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   In this work, we explore the applicability of Generative Adversarial Networks (GANs)
   to the task of generating novel works of art. Utilizing the Deep Convolutional GAN (DCGAN)
   architecture detailed by Radford \etal in \cite{radford2015unsupervised}, we train
   generative models on a dataset of 203,275 visual works of art. We show empirically that the
   DCGAN architecture is able to learn and reproduce salient features of visual artwork such as
   color, shape, composition and style. We more closely examine the representations learned by
   the generator network by employing smooth interpolation between points in its input space.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
The comprehension and production of artwork is considered by many to involve skills available
uniquely to humans. Human artists incorporate a wealth of cultural knowledge, personal experiences,
and creativity in their work. A model which could produce output human observers would accept
as novel works of art could be said to possess some representation of the artistic knowledge
that humans tap into when they create art. By examining these learned representations, we can
learn more about what they are able to encode and how they are able to encode it. In the case of convolutional neural networks, whose architecture is designed to roughly mirror the structures of neurons that make up our brains, analysis of the representations learned by such a model may even yield some insight into the way humans process visual information.

The computer-based generation of images which appear natural has in the past been limited to
the rendering of images based on models and textures carefully designed by humans. More recently,
deep learning has opened up the possibility of realistic image generation through the use of neural
networks. Goodfellow \etal \cite{goodfellow2014generative} introduced Generative Adversarial Networks
(GANs), a framework for training generative models using backpropagation. Radford \etal
\cite{radford2015unsupervised} present techniques for the effective training of Deep Convolutional
GANs (DCGANs), or deep convolutional neural networks within the GAN framework.

By training a DCGAN on a dataset of visual works of art, we create models which can generate images
which capture salient properties of artwork produced by human artists. We empirically assess the
quality of samples generated by our model and discuss the features it was able to learn.
Using analysis techniques discussed by Radford \etal in \cite{radford2015unsupervised} as well as 'deconvnets' discussed by Zeiler and Fergus in \cite{zeiler2014visualizing}, we investigate
the representations learned by our model more directly.

\section{Related Work}
\subsection{Generative Adversarial Networks}
First proposed by Goodfellow \etal in \cite{goodfellow2014generative}, GANs are a very general
framework which may be used to learn generative models of data distributions. In GANs, two models
are employed, a discriminator $ D $ and a generator $ G $. $ G $'s goal is to produce output
which mimics the features of some known data distribution $ p_{data} $, and $ D $'s goal is to distinguish
real samples drawn from $ p_{data} $ from fake samples generated by $ G $. To produce its output, $ G $
takes as input a sample $ z $ from a noise distribution $ p_{z} $. Formally, $ D $ and $ G $ play
a minimax game whose value function is defined as

\begin{equation}
\begin{aligned}
\min_{G} \max_{D}  & \mathbb{E}_{x \sim p_{data}(x)}[log(D(x))]	+ \\
				   & \mathbb{E}_{z \sim p_{z}}[log(D(G(z)))].
\end{aligned}
\end{equation}

In other words, $ D $ attempts to maximize its chances of correctly correctly classifying its input
as real or fake, and $ G $ attempts to minimize $ D $'s chances of doing so. If suitable choices
are made for the model architecture of $ D $ and $ G $, GANs may be trained using backpropagation.

\subsection{Deep Convolutional Generative Adversarial Networks}
In \cite{goodfellow2014generative}, Goodfellow \etal explored the potential for convolutional neural
networks in a GAN framework to learn a generative model of the well-known CIFAR-10 dataset
\cite{krizhevsky2014cifar}. Radford \etal expand on this idea in \cite{radford2015unsupervised},
introducing DCGANs. Their key results include a general set of guidelines for effective training of
DCGANs such as the replacement of pooling and unpooling layers with convolutional and transpose
convolutional layers respectively, the use of batch normalization layers, and the use of the LeakyReLU
activation function \cite{maas2013rectifier} in the discriminator network.

\subsection{Network Analysis Methods}
Radford \etal also discuss several methods of investigating the representations learned by the generator
network of a DCGAN. One method is walking through the latent space of $ G $'s input. If $ G $ has managed
to learn relevant and meaningful features of $ p_{data} $, then walking through the latent space will
yield smooth transitions between semantic concepts in $ G $'s output.

Another is searching for evidence that vector arithmetic in $ G $'s input space can result in meaningful
manipulation of the features of its output. For example, if $ z_{glasses} $, $ z_{man} $ and $ z_{woman} $
are noise vectors which cause $ G $ to output images of a man with glasses, a man without glasses, and a
woman without glasses respectively, then $ z_{glasses} - z_{man} + z_{woman} $ should produce an image of
a woman with glasses when passed to $ G $ as input. The presence of such structure shows that visual concepts
like glasses may be associated with vectors in $ G $'s input space.

'Deconvnets' or deconvolutional networks, presented by Zeiler and Fergus in \cite{zeiler2014visualizing}, are a technique for visualizing the representations learned by convolutional layers in convolutional neural networks. Deconvnets approach the problem of visualizing the concepts learned by high-level convolutional layers by taking a fully trained convolutional neural network and a choosing a single neuron in a higher layer. An image is passed forward through the network, and then the activations of all neurons other than the target neuron are set to 0. Then, the activation is passed backward through a deconvnet, which reconstructs the activations of the layers preceding that of the target neuron. This progressive reconstruction will eventually reach the input layer, where it will reconstruct the pattern of pixels which caused the activation of the target neuron. By visualizing the reconstructed activations of the intermediate layers and input layer, we can get a sense of the features each neuron in the network examines, as well as the patters in input which tend to activate a particular high-level neuron.


\section{Methods}
\subsection{Data}
Before attempting to train a DCGAN on a dataset of artwork, we first validated the architecture of our models by training them on the well-known MNIST dataset of handwritten digits \cite{lecun2010mnist}. The advantage to training on MNIST is that the visual features of a well-formed digit are simple and obvious, whereas the visual features of a well-formed painting are difficult to concretely define. As a result, training models on MNIST serves as a useful proof-of-concept, since visual inspection of a model's output can verify whether it is able to learn useful features of the dataset. To conduct our experiments on artwork, we compiled images of visual works of art from two sources, Kaggle and Wikiart.

Kaggle is a platform for hosting data science and machine learning competitions in which teams compete to complete a task defined by those hosting the competition. The Kaggle "Painter By Numbers" competition challenged competitors to predict whether pairs of paintings were created by the same artist. The accompanying dataset contains 79,420 images labeled with a hash of the artist's name, the title of the work, the style, the genre, and the date it was created.

Wikiart is an online encyclopedia of visual artwork maintained by users in a manner similar to Wikipedia. On their "About" page, Wikiart claims to host around 250,000 pieces of artwork from around 3,000 artists. Using a script to scrape images from their site, we were able to gather 123,854 images of paintings and some other kinds of visual artwork. Most, but not all of the images contain most of the same metadata as the Kaggle dataset. 98.29\% of images are tagged with their artist, 95.14\% are tagged with their genre, and 94.57\% are tagged with their style.

Combining these two sets of data, we managed to assemble a dataset of 203,274 paintings and other works of visual artwork, which we then used to train a DCGAN.



\subsection{Model Construction}
The model architecture we used for our experiments is based very closely on the model architecture for DCGANs presented by Radford \etal in \cite{radford2015unsupervised}. 

\section{Experiments}


\section{Conclusion}
We implement a GAN that successfully generates images with distinct attributes reminiscent of the painting set given, despite the limitations of the image resolution. Paintings generated are visually pleasing and showcase variety in color and form. Clear distinctions of genre and style can be seen in images, speaking to the ability of the network to learn stylistically consistent features and feature matching.

Future work can still easily be done on this topic. With greater processing capabilities, much higher detail can be retained in images, and more meaningful paintings could be constructed. Additionally, the dataset is ripe with contextual information that is not employed by the GAN at all. Attributes such as style, genre, and artist can easily be compiled across both databases used. It is possible that training the generator and discriminator using this metadata could aid in learning specific features or objects associated with specific attributes. 

Additionally, each painting in the database has a given title. The field of associative network generation is recent and growing as seen in \cite{ishibashi2018associative}, although it still leaves much to be desired. Painting titles may be an easier dataset to work with than sentence descriptors, as they are rarely very long strings and they are less likely to be obfuscated by complex grammar structures, org rammar structure at all. Additionally, they can still provide contextual information by describing painting content, or by conveying a mood or emotion that relates to specific color balances, styles, or forms. While there have been successful GAN implementations for text generation in \cite{zhang2017adversarial}, there have yet to be any that successfully incorporate both image and textual learning. It is also possible that the manner in which a GAN learns could enable this associative mapping to be learned more effectively than a recurrent neural network, as it builds up feature knowledge rather than extracting it from given information.

{\small
\bibliographystyle{ieee}
\bibliography{sbgan}
}

\end{document}
